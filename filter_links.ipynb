{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45f09d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# --- UTILITY FUNCTIONS ---\n",
    "\n",
    "def is_valid_url(url):\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        return parsed.scheme in (\"http\", \"https\")\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def get_domain_status(url):\n",
    "    try:\n",
    "        start = time.time()\n",
    "        response = requests.get(url, timeout=5)\n",
    "        load_time = round(time.time() - start, 2)\n",
    "        return response.status_code, load_time\n",
    "    except Exception as e:\n",
    "        return str(e), None\n",
    "\n",
    "def detect_cms(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Basic CMS fingerprints\n",
    "        if \"wp-content\" in response.text or \"WordPress\" in response.text:\n",
    "            return \"WordPress\"\n",
    "        if \"shopify\" in response.text:\n",
    "            return \"Shopify\"\n",
    "        if \"drupal\" in response.text:\n",
    "            return \"Drupal\"\n",
    "        if \"joomla\" in response.text:\n",
    "            return \"Joomla\"\n",
    "        if \"squarespace\" in response.text:\n",
    "            return \"Squarespace\"\n",
    "        if \"wix.com\" in response.text:\n",
    "            return \"Wix\"\n",
    "\n",
    "        return \"Unknown\"\n",
    "    except:\n",
    "        return \"Error\"\n",
    "\n",
    "# --- MAIN FUNCTION ---\n",
    "\n",
    "def filter_links(input_csv=\"actual_websites.csv\", output_csv=\"filtered_websites.csv\"):\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    if \"website\" not in df.columns:\n",
    "        raise ValueError(\"Input CSV must contain a 'website' column.\")\n",
    "\n",
    "    print(f\"[+] Processing {len(df)} websites for status, CMS, and speed...\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        url = row[\"website\"]\n",
    "\n",
    "        if not is_valid_url(url):\n",
    "            print(f\"[!] Skipping invalid URL: {url}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"    → Checking: {url}\")\n",
    "\n",
    "        status, speed = get_domain_status(url)\n",
    "        cms = detect_cms(url)\n",
    "\n",
    "        results.append({\n",
    "            \"website\": url,\n",
    "            \"status\": status,\n",
    "            \"cms\": cms,\n",
    "            \"load_time_sec\": speed\n",
    "        })\n",
    "\n",
    "    result_df = pd.DataFrame(results)\n",
    "    result_df.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"[✓] Saved filtered results to {output_csv}\")\n",
    "\n",
    "# --- OPTIONAL: TESTING CLI ---\n",
    "if __name__ == \"__main__\":\n",
    "    filter_links()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3702cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def save_valid_websites(input_csv, output_csv, max_load_time):\n",
    "    \"\"\"\n",
    "    Filters websites that have status 200 and load time <= max_load_time,\n",
    "    then saves them to output_csv.\n",
    "\n",
    "    Parameters:\n",
    "    - input_csv (str): Path to filtered_websites.csv file.\n",
    "    - output_csv (str): Path to save selected websites CSV.\n",
    "    - max_load_time (float): Maximum acceptable load time in seconds.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load filtered websites data\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Ensure status is numeric for filtering, coerce errors to NaN (like errors)\n",
    "    df['status'] = pd.to_numeric(df['status'], errors='coerce')\n",
    "    df['load_time_sec'] = pd.to_numeric(df['load_time_sec'], errors='coerce')\n",
    "\n",
    "    # Filter conditions: status == 200, load_time_sec <= max_load_time\n",
    "    filtered_df = df[(df['status'] == 200) & (df['load_time_sec'] <= max_load_time)]\n",
    "\n",
    "    # Save filtered valid websites\n",
    "    filtered_df.to_csv(output_csv, index=False)\n",
    "    print(f\"[✓] Saved {len(filtered_df)} valid websites to '{output_csv}'\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    save_valid_websites(\"filtered_websites.csv\", \"selected_websites.csv\", max_load_time=2.0)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
