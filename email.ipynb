{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ccc3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import tldextract\n",
    "import concurrent.futures\n",
    "\n",
    "# Email regex pattern (standard, but can be tuned)\n",
    "EMAIL_PATTERN = re.compile(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+')\n",
    "\n",
    "# Common subpage keywords to look for if no emails on homepage\n",
    "RELEVANT_SUBPAGES = ['contact', 'about', 'support', 'help', 'info', 'customer-service', 'faq']\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "\n",
    "def get_domain(url):\n",
    "    ext = tldextract.extract(url)\n",
    "    return ext.domain + '.' + ext.suffix\n",
    "\n",
    "def is_same_domain(url, base_domain):\n",
    "    domain = get_domain(url)\n",
    "    return domain == base_domain\n",
    "\n",
    "def fetch_page(url):\n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "        return resp.text\n",
    "    except Exception as e:\n",
    "        print(f\"  [!] Failed to fetch {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_emails_from_text(text):\n",
    "    emails = set(re.findall(EMAIL_PATTERN, text))\n",
    "    return emails\n",
    "\n",
    "def find_relevant_links(soup, base_url, base_domain):\n",
    "    links = []\n",
    "    for a_tag in soup.find_all('a', href=True):\n",
    "        href = a_tag['href'].lower()\n",
    "        full_url = urljoin(base_url, href)\n",
    "        if is_same_domain(full_url, base_domain):\n",
    "            for keyword in RELEVANT_SUBPAGES:\n",
    "                if keyword in href:\n",
    "                    links.append(full_url)\n",
    "                    break\n",
    "    return list(set(links))  # unique links\n",
    "\n",
    "def scrape_emails_from_url(url):\n",
    "    print(f\"[*] Scraping: {url}\")\n",
    "    html = fetch_page(url)\n",
    "    if not html:\n",
    "        return set()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    text = soup.get_text(separator=' ')\n",
    "    emails = extract_emails_from_text(text)\n",
    "    return emails\n",
    "\n",
    "def extract_emails_from_website(url):\n",
    "    base_domain = get_domain(url)\n",
    "    emails = scrape_emails_from_url(url)\n",
    "\n",
    "    if emails:\n",
    "        return emails\n",
    "\n",
    "    # No emails on homepage, try relevant subpages\n",
    "    homepage_html = fetch_page(url)\n",
    "    if not homepage_html:\n",
    "        return set()\n",
    "    soup = BeautifulSoup(homepage_html, 'html.parser')\n",
    "    relevant_links = find_relevant_links(soup, url, base_domain)\n",
    "\n",
    "    all_emails = set()\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        futures = [executor.submit(scrape_emails_from_url, link) for link in relevant_links]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            all_emails.update(future.result())\n",
    "\n",
    "    return all_emails\n",
    "\n",
    "def extract_emails_from_links(input_csv, output_csv):\n",
    "    df = pd.read_csv(input_csv)\n",
    "    results = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        website = row['website']\n",
    "        # Normalize URL (ensure scheme exists)\n",
    "        if not website.startswith(('http://', 'https://')):\n",
    "            website = 'https://' + website\n",
    "\n",
    "        emails = extract_emails_from_website(website)\n",
    "        if emails:\n",
    "            for email in emails:\n",
    "                results.append({'website': website, 'email': email})\n",
    "        else:\n",
    "            results.append({'website': website, 'email': 'no email'})\n",
    "\n",
    "    df_result = pd.DataFrame(results)\n",
    "    df_result.to_csv(output_csv, index=False)\n",
    "    print(f\"\\n[âœ“] Saved results to {output_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_emails_from_links(\"selected_websites.csv\", \"final_emails_output.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
